{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ghost Architect ‚Äî Colab T4 Main Notebook\n",
    "\n",
    "This is the single notebook to run full **Gemma-3 Trinity training** on Google Colab T4 (16GB).\n",
    "\n",
    "## What this notebook does\n",
    "1. Validates T4 runtime\n",
    "2. Installs exact dependencies\n",
    "3. Syncs project files\n",
    "4. Writes full T4 training configuration\n",
    "5. Launches training\n",
    "6. Exports GGUF artifacts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Runtime Check (must be T4 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d031deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: Tesla T4\n",
      "VRAM: 14.6 GB\n",
      "Fri Feb 27 14:02:32 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   39C    P8             13W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available(), 'CUDA is not available. Set Runtime > GPU in Colab.'\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "print(f'GPU: {gpu_name}')\n",
    "print(f'VRAM: {gpu_mem_gb:.1f} GB')\n",
    "if 'T4' not in gpu_name:\n",
    "    print('Warning: This notebook is tuned for T4; adjust config if using a different GPU.')\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26557e55",
   "metadata": {},
   "source": [
    "## 2) Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec8d8b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hNo broken requirements found.\n",
      "Dependencies installed. xformers is optional and intentionally not force-installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --retries 10 --timeout 120 --upgrade pip setuptools wheel \"jedi>=0.16\"\n",
    "!pip install -q --retries 10 --timeout 120 \"unsloth==2026.1.4\"\n",
    "!pip install -q --retries 10 --timeout 120 \"trl>=0.18.2,<=0.24.0,!=0.19.0\"\n",
    "!pip install -q --retries 10 --timeout 120 peft accelerate bitsandbytes datasets numpy scipy tqdm python-dotenv huggingface_hub pillow\n",
    "!pip install -q --retries 10 --timeout 120 \"torch>=2.1.0\" \"transformers>=4.38.0\"\n",
    "!pip check || true\n",
    "\n",
    "print('Dependencies installed. xformers is optional and intentionally not force-installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a14e54",
   "metadata": {},
   "source": [
    "## 3) Mount Drive and Sync Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c891cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find: ‚Äò/content/drive/MyDrive/ghost_architect_gemma3‚Äô: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!find /content/drive/MyDrive/ghost_architect_gemma3 -maxdepth 6 -type f -path \"*/src/train_vision.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f660fe68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/ghost_architect_gemma3\n",
      "‚úÖ Success! Current working directory: /content/drive/MyDrive/ghost_architect_gemma3\n",
      "\n",
      "Folder contents:\n",
      "configs  data  notebooks  scripts  src\n",
      "\n",
      "‚úÖ 'src/train_vision.py' found. Ready for training!\n"
     ]
    }
   ],
   "source": [
    "# 3) Mount Drive and Enter Project Folder\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Define the path to your project in Drive\n",
    "# NOTE: Ensure you uploaded the folder named 'ghost_architect_gemma3' to your Drive root\n",
    "PROJECT_DIR = '/content/drive/MyDrive/ghost_architect_gemma3'\n",
    "\n",
    "# Check if the folder exists before trying to enter it\n",
    "if not os.path.exists(PROJECT_DIR):\n",
    "    print(f\"‚ùå Error: Could not find project at {PROJECT_DIR}\")\n",
    "    print(\"Please make sure you dragged the 'ghost_architect_gemma3' folder to the main 'My Drive' screen.\")\n",
    "    print(\"Listing folders in your Drive to help debug:\")\n",
    "    !ls /content/drive/MyDrive | head -n 10\n",
    "else:\n",
    "    # Change directory directly INTO the Drive folder\n",
    "    %cd {PROJECT_DIR}\n",
    "    \n",
    "    # Verify we are in the right place\n",
    "    print(f\"‚úÖ Success! Current working directory: {os.getcwd()}\")\n",
    "    print(\"\\nFolder contents:\")\n",
    "    !ls\n",
    "    \n",
    "    # Verify the critical Vision Trainer exists\n",
    "    if os.path.exists('src/train_vision.py'):\n",
    "        print(\"\\n‚úÖ 'src/train_vision.py' found. Ready for training!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Warning: 'src/train_vision.py' not found. Did you upload the 'src' folder?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f350dcba",
   "metadata": {},
   "source": [
    "## 3.5) Environment Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94a789b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Ghost Architect Environment Validation\n",
      "==================================================\n",
      "üêç Checking Python version...\n",
      "   ‚úÖ Python 3.12.12 (compatible)\n",
      "\n",
      "üì¶ Checking dependencies...\n",
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2026-02-27 14:04:37.797584: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1772201077.818176    1313 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1772201077.824970    1313 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1772201077.842293    1313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772201077.842319    1313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772201077.842323    1313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772201077.842327    1313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-27 14:04:37.846921: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "   ‚úÖ unsloth\n",
      "   ‚úÖ torch\n",
      "   ‚úÖ transformers\n",
      "   ‚úÖ accelerate\n",
      "   ‚úÖ bitsandbytes\n",
      "   ‚úÖ peft\n",
      "   ‚úÖ trl\n",
      "   ‚úÖ datasets\n",
      "\n",
      "‚ö° Checking Unsloth compatibility...\n",
      "   ‚úÖ Unsloth imported successfully\n",
      "   ‚úÖ Ready for Gemma-3-12B training\n",
      "\n",
      "üî• Checking GPU availability...\n",
      "   ‚úÖ CUDA available with 1 GPU(s)\n",
      "   üìü GPU 0: Tesla T4 (14.6GB)\n",
      "   ‚ö†Ô∏è  Warning: GPU 0 has less than 15GB VRAM (may need reduced settings)\n",
      "\n",
      "üß† Memory usage estimation (Trinity architecture):\n",
      "   üìä Model weights (4-bit QLoRA): 7.6GB\n",
      "   üìä Gradients (Rank 64 + DoRA): 5.5GB\n",
      "   üìä Context overhead (4096 seq): 2.5GB\n",
      "   üìä System buffer: 0.4GB\n",
      "   üìä Total estimated: 15.6GB\n",
      "\n",
      "üí° Recommendations:\n",
      "   ‚ö†Ô∏è  May need: max_seq_length=2048, rank=32\n",
      "\n",
      "==================================================\n",
      "üéâ Environment validation PASSED!\n",
      "   Ready to begin Phase 1: Trinity training\n"
     ]
    }
   ],
   "source": [
    "!python scripts/validate_environment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06865c9c",
   "metadata": {},
   "source": [
    "## 4) Full T4 Trinity Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da84101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote configs/training_config_colab_t4.yaml\n"
     ]
    }
   ],
   "source": [
    "config_yaml = '''\n",
    "model_name: \"unsloth/gemma-3-12b-it-bnb-4bit\"\n",
    "max_seq_length: 4096\n",
    "load_in_4bit: true\n",
    "\n",
    "lora:\n",
    "  r: 64\n",
    "  lora_alpha: 32\n",
    "  target_modules:\n",
    "    - q_proj\n",
    "    - k_proj\n",
    "    - v_proj\n",
    "    - o_proj\n",
    "    - gate_proj\n",
    "    - up_proj\n",
    "    - down_proj\n",
    "  use_rslora: true\n",
    "  use_dora: true\n",
    "  lora_dropout: 0.1\n",
    "\n",
    "training:\n",
    "  per_device_train_batch_size: 1\n",
    "  gradient_accumulation_steps: 4\n",
    "  learning_rate: 2e-4\n",
    "  max_steps: 60\n",
    "  warmup_steps: 10\n",
    "  logging_steps: 1\n",
    "  save_steps: 20\n",
    "  optimizer: \"adamw_8bit\"\n",
    "  lr_scheduler_type: \"cosine\"\n",
    "\n",
    "output:\n",
    "  adapters_dir: \"output/adapters\"\n",
    "  checkpoints_dir: \"output/checkpoints\"\n",
    "  gguf_dir: \"output/gguf\"\n",
    "\n",
    "oom_fallbacks:\n",
    "  - {action: reduce_seq_len, value: 2048}\n",
    "  - {action: reduce_rank, value: 32}\n",
    "  - {action: disable_dora, value: false}\n",
    "'''\n",
    "\n",
    "os.makedirs('configs', exist_ok=True)\n",
    "with open('configs/training_config_colab_t4.yaml', 'w') as f:\n",
    "    f.write(config_yaml)\n",
    "\n",
    "print('Wrote configs/training_config_colab_t4.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48880cd5",
   "metadata": {},
   "source": [
    "## 5) Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81e51c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset valid: 287 vision examples\n",
      "   First example domain: trigger.io_77153\n",
      "   Image path exists: True\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "dataset_path = 'data/dataset_vision.json'\n",
    "assert os.path.exists(dataset_path), f'Missing dataset: {dataset_path}'\n",
    "\n",
    "with open(dataset_path, 'r') as f:\n",
    "    examples = json.load(f)\n",
    "\n",
    "print(f'‚úÖ Dataset valid: {len(examples)} vision examples')\n",
    "print(f'   First example domain: {examples[0][\"domain\"]}')\n",
    "print(f'   Image path exists: {os.path.exists(examples[0][\"image_path\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f015f69",
   "metadata": {},
   "source": [
    "## 6) Launch Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dora-dtype-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DoRA dtype patch ‚Äî fixes PEFT bug where DoRA passes fp16 x_eye to fp32 lora_A\n",
    "# without casting. Unsloth's Gemma3 patch intentionally runs q_proj in fp16,\n",
    "# which exposes this missing cast in peft/tuners/lora/dora.py.\n",
    "# Regular LoRA does x = x.to(lora_A.weight.dtype); DoRA skips it.\n",
    "# This patch adds the missing cast so DoRA works on T4 with Gemma3.\n",
    "import peft.tuners.lora.dora as _dora_mod\n",
    "\n",
    "_dora_path = _dora_mod.__file__\n",
    "with open(_dora_path, 'r') as _f:\n",
    "    _src = _f.read()\n",
    "\n",
    "_old = '        lora_weight = lora_B(lora_A(x_eye)).T'\n",
    "_new = (\n",
    "    '        x_eye = x_eye.to(next(lora_A.parameters()).dtype)  '\n",
    "    '# cast fp16‚Üífp32 to match lora_A weights\\n'\n",
    "    '        lora_weight = lora_B(lora_A(x_eye)).T'\n",
    ")\n",
    "\n",
    "if _old not in _src:\n",
    "    print('‚ö†Ô∏è  DoRA patch: target line not found ‚Äî already patched or PEFT version changed')\n",
    "else:\n",
    "    _src = _src.replace(_old, _new, 1)\n",
    "    with open(_dora_path, 'w') as _f:\n",
    "        _f.write(_src)\n",
    "    print(f'‚úÖ DoRA patched at {_dora_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9879bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2026-02-27 15:44:54.166009: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1772207094.201567   27778 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1772207094.213345   27778 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1772207094.241492   27778 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772207094.241531   27778 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772207094.241538   27778 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772207094.241545   27778 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-27 15:44:54.248629: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.1.4: Fast Gemma3 patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.35. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n",
      "Loading checkpoint shards: 100% 3/3 [00:50<00:00, 16.97s/it]\n",
      "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n",
      "Unsloth: Switching to float32 training since model cannot work with float16\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 287 | Num Epochs = 1 | Total steps = 72\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 264,118,272 of 12,451,443,312 (2.12% trained)\n",
      "  0% 0/72 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/ghost_architect_gemma3/src/train_vision.py\", line 172, in <module>\n",
      "    train_vision_model(args.dataset, args.output)\n",
      "  File \"/content/drive/MyDrive/ghost_architect_gemma3/src/train_vision.py\", line 160, in train_vision_model\n",
      "    trainer.train()\n",
      "  File \"/tmp/unsloth_compiled_cache/UnslothSFTTrainer.py\", line 65, in wrapper\n",
      "    output = f(self, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 2325, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<string>\", line 328, in _fast_inner_training_loop\n",
      "  File \"/tmp/unsloth_compiled_cache/UnslothSFTTrainer.py\", line 1262, in training_step\n",
      "    return super().training_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<string>\", line 40, in _unsloth_training_step\n",
      "  File \"/tmp/unsloth_compiled_cache/UnslothSFTTrainer.py\", line 1251, in compute_loss\n",
      "    outputs = super().compute_loss(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\", line 1661, in _unsloth_pre_compute_loss\n",
      "    outputs = self._old_compute_loss(model, inputs, *args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<string>\", line 36, in compute_loss\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\", line 1923, in forward\n",
      "    return self.base_model(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py\", line 311, in forward\n",
      "    return self.model.forward(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py\", line 902, in forward\n",
      "    return Gemma3ForConditionalGeneration_forward(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **lm_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py\", line 203, in nonrecursive_disable_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py\", line 712, in Gemma3ForConditionalGeneration_forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 918, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/gemma3/modeling_gemma3.py\", line 957, in forward\n",
      "    outputs = self.language_model(\n",
      "              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 1072, in wrapper\n",
      "    outputs = func(self, *args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/gemma3/modeling_gemma3.py\", line 570, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\", line 93, in __call__\n",
      "    return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_compile.py\", line 54, in inner\n",
      "    return disable_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\", line 1181, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py\", line 505, in checkpoint\n",
      "    return CheckpointFunction.apply(function, preserve, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\", line 583, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/unsloth_zoo/gradient_checkpointing.py\", line 498, in forward\n",
      "    outputs = run_function(*args)\n",
      "              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/gemma3/modeling_gemma3.py\", line 382, in forward\n",
      "    hidden_states, self_attn_weights = self.self_attn(\n",
      "                                       ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/unsloth_zoo/temporary_patches/gemma.py\", line 550, in forward\n",
      "    return forward_function(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/unsloth_zoo/temporary_patches/gemma.py\", line 412, in forward_function\n",
      "    query_states_fp16 = self.q_proj(hidden_states) # output fp16\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/unsloth_compiled_cache/Linear4bit_peft_forward.py\", line 113, in unsloth_forward\n",
      "    result = self.lora_variant[active_adapter].forward(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/variants.py\", line 232, in forward\n",
      "    result = result + module.lora_magnitude_vector[active_adapter](\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/dora.py\", line 74, in forward\n",
      "    lora_weight = lora_B(lora_A(x_eye)).T\n",
      "                         ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: expected mat1 and mat2 to have the same dtype, but got: c10::Half != float\n",
      "  0%|          | 0/72 [00:12<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Clear stale compiled cache so Unsloth recompiles with current TRL version\n",
    "!rm -rf ghost_architect_gemma3/unsloth_compiled_cache\n",
    "\n",
    "!python src/train_vision.py --dataset data/dataset_vision.json --output output/adapters/phase2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Export to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/export.py \\\n",
    "    --adapter_dir output/adapters/phase2 \\\n",
    "    --output_dir output/gguf \\\n",
    "    --quantization q4_k_m\n",
    "\n",
    "# After this finishes, your model is at: output/gguf/ghost-architect-v1.gguf\n",
    "# Download it from Colab: Files panel (left sidebar) ‚Üí output/gguf/\n",
    "# Then run locally: ollama run ./ghost-architect-v1.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55a56c5",
   "metadata": {},
   "source": [
    "## 8) Launch Ghost Architect Web App (Streamlit + Tunnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streamlit-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launches the Ghost Architect Streamlit app from Colab and exposes it\n",
    "# via a public tunnel URL so you can access it from your browser.\n",
    "#\n",
    "# After training completes, run this cell.\n",
    "# You will get a URL (ending in .loca.lt) and a password (your public IP).\n",
    "# Open the URL, enter the password, and the app is live.\n",
    "import subprocess, urllib, time\n",
    "\n",
    "!pip install -q streamlit\n",
    "!npm install -q localtunnel\n",
    "\n",
    "# Start Streamlit in background (points at the Modal-downloaded or Colab-trained adapter)\n",
    "subprocess.Popen([\n",
    "    'streamlit', 'run', 'src/app.py',\n",
    "    '--server.port', '8501',\n",
    "    '--server.headless', 'true',\n",
    "])\n",
    "time.sleep(3)  # wait for Streamlit to boot\n",
    "\n",
    "# Get your public IP ‚Äî this is the tunnel password\n",
    "public_ip = urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode().strip()\n",
    "print(f'\\nüîë Tunnel password (paste when prompted): {public_ip}')\n",
    "print('üëá Click the link below then enter the password above:')\n",
    "\n",
    "# Open tunnel ‚Äî blocks until you Ctrl+C\n",
    "!npx localtunnel --port 8501\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
