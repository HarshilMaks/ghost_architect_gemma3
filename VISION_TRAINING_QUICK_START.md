# ğŸš€ Ghost Architect Phase 2: Vision Training Quick Start

## What Just Happened âœ…

You now have a **production-grade UI-to-SQL vision training pipeline**:

### 1. Dataset Created
âœ… **data/dataset_vision.json** â€” 136 annotated UI screenshots
- Domain-classified (dashboards, e-commerce, admin, portfolio, chat)
- SQL schemas auto-generated based on visual context
- Ready for Gemma-3 vision model training

### 2. Training Scripts Ready
âœ… **src/train_vision_prod.py** â€” Production training with Trinity architecture
- QLoRA (4-bit quantization) = fits on T4 GPU
- DoRA (weight decomposition) = better convergence
- rsLoRA (rank stabilization) = enables rank=64 safely

âœ… **scripts/build_vision_dataset.py** â€” Validates & prepares images
- Deduplication via content hash
- Image quality validation
- Auto-classification by domain

### 3. Documentation Complete
âœ… **PHASE2_VISION_TRAINING.md** â€” Full production guide
- Step-by-step Colab instructions
- Memory budget breakdown
- Troubleshooting guide

---

## Next Steps: Train on Your GPU

### Option A: Google Colab (Recommended)
```python
# In Colab cell:
!python src/train_vision_prod.py --dataset data/dataset_vision.json
```

Expected:
- â±ï¸ Duration: 45-90 minutes on T4
- ğŸ¯ VRAM: 14.8-15.6 GB (98% usage = good!)
- ğŸ“Š Output: `output/adapters/vision_trinity/`

### Option B: Local GPU
```bash
# Local terminal:
python src/train_vision_prod.py --dataset data/dataset_vision.json
```

### Option C: Test Run (Small Dataset)
```bash
python src/train_vision_prod.py --dataset data/dataset_vision.json --max-examples 20
```

---

## What Training Does

### Teaches the model:
âœ… Understand UI layouts (buttons, forms, tables, charts)
âœ… Identify data structures from visual elements
âœ… Generate SQL schemas for different UI patterns
âœ… Recognize domain-specific UI components

### Outputs:
ğŸ“¦ LoRA adapters (8-15 MB lightweight)
ğŸ“¦ Training logs with loss metrics
ğŸ“¦ Ready for GGUF export

---

## Production Pipeline After Training

```
1. Train Vision Model
   â†“
2. Export to GGUF
   â†“
3. Deploy with Ollama
   â†“
4. Production API
```

---

## Key Files

| File | What it does |
|------|-------------|
| `data/dataset_vision.json` | 136 annotated UI training examples |
| `src/train_vision_prod.py` | Main training script (production-ready) |
| `scripts/build_vision_dataset.py` | Dataset builder from screenshots |
| `PHASE2_VISION_TRAINING.md` | Full detailed guide |

---

## Training Checklist

Before you start:
- [ ] `.env` file has `HUGGINGFACE_TOKEN`
- [ ] Ran `scripts/build_vision_dataset.py` (âœ… done)
- [ ] Dataset file exists: `data/dataset_vision.json`
- [ ] GPU available (T4 on Colab or local)

---

## Troubleshooting Quick Answers

**Q: Out of VRAM?**
A: Run `--max-examples 50` for smaller test run

**Q: HuggingFace auth error?**
A: Visit https://huggingface.co/google/gemma-3-12b-it and click "Access Repository"

**Q: Training too slow?**
A: Normal on T4. Grab coffee â˜• (~1 hour is expected)

---

## You're Ready! ğŸ‰

The foundation for **production-grade UI-to-SQL vision AI** is set up.

**Next command:**
```bash
python src/train_vision_prod.py --dataset data/dataset_vision.json
```

Good luck! ğŸš€
